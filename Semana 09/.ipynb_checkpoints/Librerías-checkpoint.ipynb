{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.isdir(\"./libs\"):\n",
    "    os.mkdir(\"./libs\")\n",
    "if os.path.isdir(\"./CPAT\"):\n",
    "    shutil.rmtree(\"./CPAT\")\n",
    "if os.path.isdir(\"./Diamond\"):\n",
    "    shutil.rmtree(\"./Diamond\")\n",
    "if os.path.isdir(\"./features\"):\n",
    "    shutil.rmtree(\"./features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./libs/util_bd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./libs/util_bd.py\n",
    "import mysql.connector\n",
    "import myloginpath\n",
    "import pandas as pd\n",
    "\n",
    "def resultados_query(query):\n",
    "    conf = myloginpath.parse('tesis2')\n",
    "    conn = mysql.connector.connect(**conf, db=\"tesis2\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    resultado = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return resultado\n",
    "\n",
    "def ejecutar_query(query):\n",
    "    conf = myloginpath.parse('tesis2')\n",
    "    conn = mysql.connector.connect(**conf, db=\"tesis2\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    conn.close()\n",
    "\n",
    "def mostrar_resultado_query(query):\n",
    "    conf = myloginpath.parse('tesis2')\n",
    "    conn = mysql.connector.connect(**conf, db=\"tesis2\")\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    display(df)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./libs/util_fasta.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./libs/util_fasta.py\n",
    "import os\n",
    "\n",
    "def generar_fasta(secuencias, archivo, tamanio_por_linea=80):\n",
    "    t_tamanio = tamanio_por_linea\n",
    "    f = open(archivo ,\"w+\")\n",
    "    for transcrito in secuencias:\n",
    "        f.write(\">%s\\n\" % (transcrito[0].strip().upper()))\n",
    "        seq = transcrito[1]\n",
    "        t_partes = [seq[i:i+t_tamanio] for i in range(0, len(seq), t_tamanio)]\n",
    "        for t_parte in t_partes:\n",
    "            f.write(\"%s\\n\" % (t_parte))\n",
    "    f.close()\n",
    "\n",
    "def leer_fasta(archivo):\n",
    "    transcritos = {}\n",
    "    cod_secuencia = \"\"\n",
    "    secuencia = \"\"\n",
    "    f = open(archivo, \"r\")\n",
    "    for linea in f:\n",
    "        if linea.startswith(\">\"):\n",
    "            if secuencia != \"\":\n",
    "                transcritos[cod_secuencia] = secuencia\n",
    "                secuencia = \"\"\n",
    "            cod_secuencia = linea.rstrip(\"\\n\").lstrip(\">\").strip().upper()\n",
    "        else:\n",
    "            secuencia += linea.rstrip(\"\\n\")\n",
    "    if secuencia != \"\":\n",
    "        transcritos[cod_secuencia] = secuencia\n",
    "        secuencia = \"\"\n",
    "    f.close()\n",
    "    return transcritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./libs/util_caracteristicas.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./libs/util_caracteristicas.py\n",
    "import os\n",
    "import util_bd, util_fasta\n",
    "from Bio.SeqUtils import GC\n",
    "import csv\n",
    "from sklearn.externals.joblib import dump, load\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "def generar_modelo_CPAT(identificador, codigos_lncRNA, codigos_PCT):\n",
    "    archivos = _rutas_archivos(identificador)\n",
    "    if os.path.isdir(archivos[\"cpat\"][\"directorio_base\"]):\n",
    "        return\n",
    "    _generar_directorios_cpat(archivos)\n",
    "    _generar_data_cpat(archivos, codigos_lncRNA, codigos_PCT)\n",
    "    _generar_modelo_cpat(archivos)\n",
    "\n",
    "def generar_caracteristicas(identificador, transcritos):\n",
    "    archivos = _rutas_archivos(identificador)\n",
    "    if not os.path.isdir(archivos[\"cpat\"][\"directorio_base\"]):\n",
    "        raise Exception(\"No se encontró la carpeta del modelo CPAT {}, probablemente aún no ha generado este modelo. Ruta buscada: {}\".format(identificador, archivos[\"cpat\"][\"directorio_base\"]))\n",
    "    _generar_transcritos_fasta(archivos, transcritos)\n",
    "    _generar_caracteristicas_cpat(archivos, archivos[\"transcritos_fasta\"])\n",
    "    _generar_caracteristicas_diamond(archivos, archivos[\"transcritos_fasta\"])\n",
    "    _generar_caracteristicas(archivos, transcritos)\n",
    "    _limpiar_archivos_cpat(archivos)\n",
    "\n",
    "def generar_caracteristicas_cpat(identificador, transcritos):\n",
    "    archivos = _rutas_archivos(identificador)\n",
    "    if not os.path.isdir(archivos[\"cpat\"][\"directorio_base\"]):\n",
    "        raise Exception(\"No se encontró la carpeta del modelo CPAT {}, probablemente aún no ha generado este modelo. Ruta buscada: {}\".format(identificador, archivos[\"cpat\"][\"directorio_base\"]))\n",
    "    _generar_transcritos_fasta(archivos, transcritos)\n",
    "    _generar_caracteristicas_cpat(archivos, archivos[\"transcritos_fasta\"])\n",
    "    _limpiar_archivos_cpat(archivos)\n",
    "    \n",
    "def existe_modelo_cpat(identificador):\n",
    "    archivos = _rutas_archivos(identificador)\n",
    "    return os.path.isdir(archivos[\"cpat\"][\"directorio_base\"])\n",
    "    \n",
    "def _rutas_archivos(identificador):\n",
    "    if not os.path.isdir(\"./CPAT\"):\n",
    "        os.mkdir(\"./CPAT\")\n",
    "    if not os.path.isdir(\"./Diamond\"):\n",
    "        os.mkdir(\"./Diamond\")\n",
    "    if not os.path.isdir(\"./features\"):\n",
    "        os.mkdir(\"./features\")\n",
    "    \n",
    "    archivos = {}\n",
    "    archivos[\"cpat\"] = { \"directorio_base\" : \"./CPAT/{}\".format(identificador) }\n",
    "    archivos[\"cpat\"][\"data\"] = { \"directorio\" : \"{}/data\".format(archivos[\"cpat\"][\"directorio_base\"]) }\n",
    "    archivos[\"cpat\"][\"data\"][\"lncRNA\"] = \"{}/lncRNA.fasta\".format(archivos[\"cpat\"][\"data\"][\"directorio\"])\n",
    "    archivos[\"cpat\"][\"data\"][\"PCT\"] = \"{}/PCT.fasta\".format(archivos[\"cpat\"][\"data\"][\"directorio\"])\n",
    "    archivos[\"cpat\"][\"data\"][\"CDS\"] = \"{}/CDS.fasta\".format(archivos[\"cpat\"][\"data\"][\"directorio\"])\n",
    "    archivos[\"cpat\"][\"modelo\"] = { \"directorio\" : \"{}/modelo\".format(archivos[\"cpat\"][\"directorio_base\"]) }\n",
    "    archivos[\"cpat\"][\"modelo\"][\"hexamer\"] = \"{}/hexamer.tsv\".format(archivos[\"cpat\"][\"modelo\"][\"directorio\"])\n",
    "    archivos[\"cpat\"][\"modelo\"][\"prefijo_logit\"] = \"{}/{}\".format(archivos[\"cpat\"][\"modelo\"][\"directorio\"], identificador)\n",
    "    archivos[\"cpat\"][\"modelo\"][\"logit\"] = \"{}.logit.RData\".format(archivos[\"cpat\"][\"modelo\"][\"prefijo_logit\"])\n",
    "    archivos[\"cpat\"][\"modelo\"][\"prefijo_cpat\"] = \"{}/{}\".format(archivos[\"cpat\"][\"modelo\"][\"directorio\"], identificador)\n",
    "    archivos[\"cpat\"][\"salida\"] = \"{}.dat\".format(archivos[\"cpat\"][\"modelo\"][\"prefijo_cpat\"])\n",
    "    archivos[\"cpat\"][\"scripts\"] = {\n",
    "        \"script_hexamer\" : \"~/anaconda3/bin/make_hexamer_tab.py\",\n",
    "        \"script_logit\" : \"~/anaconda3/bin/make_logitModel.py\",\n",
    "        \"script_cpat\" : \"~/anaconda3/bin/cpat.py\"\n",
    "    }\n",
    "    archivos[\"diamond\"] = { \"directorio_base\" : \"./Diamond\" }\n",
    "    archivos[\"diamond\"][\"bd\"] = \"{}_BD/uniprot-viridiplantae-reviewed.dmnd\".format(archivos[\"diamond\"][\"directorio_base\"])\n",
    "    archivos[\"diamond\"][\"script\"] = \"~/anaconda3/bin/diamond\"\n",
    "    archivos[\"diamond\"][\"salida\"] = \"{}/{}.tsv\".format(archivos[\"diamond\"][\"directorio_base\"], identificador)\n",
    "    archivos[\"transcritos_fasta\"] = \"./data/{}.fasta\".format(identificador)\n",
    "    archivos[\"features\"] = { \"directorio_base\" : \"./features\"}\n",
    "    archivos[\"features\"][\"salida\"] = \"{}/{}.feat\".format(archivos[\"features\"][\"directorio_base\"], identificador)\n",
    "    return archivos\n",
    "    \n",
    "def _generar_directorios_cpat(archivos):\n",
    "    os.mkdir(archivos[\"cpat\"][\"directorio_base\"])\n",
    "    os.mkdir(archivos[\"cpat\"][\"data\"][\"directorio\"])\n",
    "    os.mkdir(archivos[\"cpat\"][\"modelo\"][\"directorio\"])\n",
    "\n",
    "def _limpiar_archivos_cpat(archivos):\n",
    "    pass #shutil.rmtree(archivos[\"cpat\"][\"data\"])\n",
    "\n",
    "def _generar_data_cpat(archivos, codigos_lncRNA, codigos_PCT):\n",
    "    query = \"SELECT cod_secuencia, secuencia FROM secuencias WHERE cod_secuencia IN ('{}')\".format(\"', '\".join(codigos_lncRNA))\n",
    "    secuencias = util_bd.resultados_query(query)\n",
    "    util_fasta.generar_fasta(secuencias, archivos[\"cpat\"][\"data\"][\"lncRNA\"])\n",
    "    query = \"SELECT cod_secuencia, secuencia FROM secuencias WHERE cod_secuencia IN ('{}')\".format(\"', '\".join(codigos_PCT))\n",
    "    secuencias = util_bd.resultados_query(query)\n",
    "    util_fasta.generar_fasta(secuencias, archivos[\"cpat\"][\"data\"][\"PCT\"])\n",
    "    query = \"SELECT cod_secuencia, coding FROM secuencias_CDS WHERE cod_secuencia IN ('{}')\".format(\"', '\".join(codigos_PCT))\n",
    "    secuencias = util_bd.resultados_query(query)\n",
    "    util_fasta.generar_fasta(secuencias, archivos[\"cpat\"][\"data\"][\"CDS\"])\n",
    "\n",
    "def _generar_modelo_cpat(archivos):\n",
    "    _generar_hexamer_cpat(archivos)\n",
    "    _generar_logit_cpat(archivos)\n",
    "    \n",
    "def _generar_hexamer_cpat(archivos):\n",
    "    script = archivos[\"cpat\"][\"scripts\"][\"script_hexamer\"]\n",
    "    fasta_cds = \"'\" + archivos[\"cpat\"][\"data\"][\"CDS\"] + \"'\" \n",
    "    fasta_lncRNA = \"'\" + archivos[\"cpat\"][\"data\"][\"lncRNA\"] + \"'\"\n",
    "    salida = \"'\" + archivos[\"cpat\"][\"modelo\"][\"hexamer\"] + \"'\"\n",
    "    comando = \"{} -c {} -n {} > {}\".format(script, fasta_cds, fasta_lncRNA, salida)\n",
    "    os.system(comando)\n",
    "    \n",
    "def _generar_logit_cpat(archivos):\n",
    "    script = archivos[\"cpat\"][\"scripts\"][\"script_logit\"]\n",
    "    hexamer = \"'\" + archivos[\"cpat\"][\"modelo\"][\"hexamer\"] + \"'\"\n",
    "    fasta_pct = \"'\" + archivos[\"cpat\"][\"data\"][\"PCT\"] + \"'\" \n",
    "    fasta_lncRNA = \"'\" + archivos[\"cpat\"][\"data\"][\"lncRNA\"] + \"'\"\n",
    "    salida = \"'\" + archivos[\"cpat\"][\"modelo\"][\"prefijo_logit\"] + \"'\"\n",
    "    comando = \"{} -x {} -c {} -n {} -o {}\".format(script, hexamer, fasta_pct, fasta_lncRNA, salida)\n",
    "    os.system(comando)\n",
    "\n",
    "def _generar_transcritos_fasta(archivos, transcritos):\n",
    "    transcritos_array = transcritos.items()\n",
    "    util_fasta.generar_fasta(transcritos_array, archivos[\"transcritos_fasta\"])\n",
    "    \n",
    "def _generar_caracteristicas_cpat(archivos, transcritos_fasta):\n",
    "    script = archivos[\"cpat\"][\"scripts\"][\"script_cpat\"]\n",
    "    logit = \"'\" + archivos[\"cpat\"][\"modelo\"][\"logit\"] + \"'\"\n",
    "    hexamer = \"'\" + archivos[\"cpat\"][\"modelo\"][\"hexamer\"] + \"'\"\n",
    "    salida = \"'\" + archivos[\"cpat\"][\"modelo\"][\"prefijo_cpat\"] + \"'\"\n",
    "    comando = \"{} -g {} -d {} -x {} -o {}\".format(script, transcritos_fasta, logit, hexamer, salida)\n",
    "    os.system(comando)\n",
    "\n",
    "def _generar_caracteristicas_diamond(archivos, transcritos_fasta):\n",
    "    script = archivos[\"diamond\"][\"script\"]\n",
    "    diamond_bd = \"'\" + archivos[\"diamond\"][\"bd\"] + \"'\"\n",
    "    salida = \"'\" + archivos[\"diamond\"][\"salida\"] + \"'\"\n",
    "    comando = \"{} blastx -d {} -q {} -o {} -k 5 --gapopen 11 --gapextend 1 --more-sensitive -f 6 qseqid pident length qframe qstart qend sstart send evalue bitscore\".format(script, diamond_bd, transcritos_fasta, salida)\n",
    "    os.system(comando)\n",
    "\n",
    "def _generar_caracteristicas(archivos, transcritos):\n",
    "    transcript_dict = {}\n",
    "    for k in transcritos.keys():\n",
    "        transcript_dict[k.strip().upper()] = {\n",
    "            \"length\" : len(transcritos[k]),\n",
    "            \"gc\" : GC(transcritos[k]),\n",
    "            \"orf_length\" : 0,\n",
    "            \"orf_coverage\" : float(0),\n",
    "            \"hexamer_score\" : float(0),\n",
    "            \"fickett_score\" : float(0),\n",
    "            \"identity\" : float(0),\n",
    "            \"align_length\" : float(0),\n",
    "            \"align_perc_len\" : float(0),\n",
    "            \"align_perc_orf\" : float(0)\n",
    "        }\n",
    "    \n",
    "    with open(archivos[\"cpat\"][\"salida\"], \"r\") as f:\n",
    "        cpat_reader = csv.reader(f, delimiter=(\"\\t\"))\n",
    "        for row in cpat_reader:\n",
    "            cod_secuencia = row[0]\n",
    "            transcript_dict[cod_secuencia][\"orf_length\"] = float(row[2])\n",
    "            transcript_dict[cod_secuencia][\"orf_coverage\"] = float(row[2])/float(transcript_dict[cod_secuencia][\"length\"])\n",
    "            transcript_dict[cod_secuencia][\"fickett_score\"] = float(row[3])\n",
    "            transcript_dict[cod_secuencia][\"hexamer_score\"] = float(row[4])\n",
    "    \n",
    "    #adaptado de https://github.com/gbgolding/crema/blob/master/bin/featuresetup_module.py\n",
    "    with open(archivos[\"diamond\"][\"salida\"], \"r\") as f:\n",
    "        tab_reader = csv.reader(f, delimiter=(\"\\t\"))\n",
    "        line_1 = next(tab_reader)\n",
    "        first = line_1[0].upper()\n",
    "        score = [float(line_1[9])]\n",
    "        with_len = [[first, float(line_1[1]), float(line_1[2]), float(line_1[3]), float(line_1[9])]] # name identity length frame score\n",
    "        for row in tab_reader:\n",
    "            if row[0].upper() == first:\n",
    "                score.append(float(row[9]))\n",
    "                with_len.append([row[0].upper(), float(row[1]), float(row[2]), float(row[3]), float(row[9])])\n",
    "            else:\n",
    "                transcript_dict[first][\"identity\"] = float(0)\n",
    "                transcript_dict[first][\"align_length\"] = float(0)\n",
    "                max_value = max(score)\n",
    "                max_index = score.index(max_value)\n",
    "                max_len_ident = with_len[max_index]\n",
    "                if max_len_ident[3] > 0:\n",
    "                    transcript_dict[first][\"identity\"] = float(max_len_ident[1])\n",
    "                    transcript_dict[first][\"align_length\"] = float(max_len_ident[2])\n",
    "                    transcript_dict[first][\"align_perc_len\"] = float(transcript_dict[first][\"align_length\"]/transcript_dict[first][\"length\"])\n",
    "                    transcript_dict[first][\"align_perc_orf\"] = (0 if transcript_dict[first][\"orf_length\"] == 0 else float(transcript_dict[first][\"align_length\"]/transcript_dict[first][\"orf_length\"]))\n",
    "                score = [float(row[9])]\n",
    "                first = row[0].upper()\n",
    "                with_len = [[first, float(row[1]), float(row[2]), float(row[3]), float(row[9])]]\n",
    "        transcript_dict[first][\"identity\"] = float(0)\n",
    "        transcript_dict[first][\"align_length\"] = float(0)\n",
    "        max_value = max(score)\n",
    "        max_index = score.index(max_value)\n",
    "        max_len_ident = with_len[max_index]\n",
    "        if max_len_ident[3] > 0:\n",
    "            transcript_dict[first][\"identity\"] = float(max_len_ident[1])\n",
    "            transcript_dict[first][\"align_length\"] = float(max_len_ident[2])\n",
    "    #fin de código adaptado de https://github.com/gbgolding/crema/blob/master/bin/featuresetup_module.py\n",
    "    \n",
    "    dump(transcript_dict, archivos[\"features\"][\"salida\"])\n",
    "\n",
    "def obtener_caracteristicas(identificador, id_cpat, transcritos):\n",
    "    archivos = _rutas_archivos(identificador)\n",
    "    archivos_cpat = _rutas_archivos(id_cpat)\n",
    "    if not os.path.isfile(archivos[\"features\"][\"salida\"]):\n",
    "        raise Exception(\"Debe primero generar la base de datos de caracteristicas \" + identificador)\n",
    "    if not os.path.isfile(archivos_cpat[\"cpat\"][\"salida\"]):\n",
    "        raise Exception(\"Debe primero generar la base de datos de caracteristicas para CPAT \" + id_cpat)\n",
    "    \n",
    "    features_globales = load(archivos[\"features\"][\"salida\"])\n",
    "    transcript_dict = {}\n",
    "    for k in transcritos.keys():\n",
    "        transcript_dict[k.strip().upper()] = {\n",
    "            \"length\" : features_globales[k.strip().upper()][\"length\"],\n",
    "            \"gc\" : features_globales[k.strip().upper()][\"gc\"],\n",
    "            \"orf_length\" : features_globales[k.strip().upper()][\"orf_length\"],\n",
    "            \"orf_coverage\" : features_globales[k.strip().upper()][\"orf_coverage\"],\n",
    "            \"hexamer_score\" : float(0),\n",
    "            \"fickett_score\" : float(0),\n",
    "            \"identity\" : features_globales[k.strip().upper()][\"identity\"],\n",
    "            \"align_length\" : features_globales[k.strip().upper()][\"align_length\"],\n",
    "            \"align_perc_len\" : features_globales[k.strip().upper()][\"align_perc_len\"],\n",
    "            \"align_perc_orf\" : features_globales[k.strip().upper()][\"align_perc_orf\"]\n",
    "        }\n",
    "    \n",
    "    with open(archivos_cpat[\"cpat\"][\"salida\"], \"r\") as f:\n",
    "        cpat_reader = csv.reader(f, delimiter=(\"\\t\"))\n",
    "        for row in cpat_reader:\n",
    "            cod_secuencia = row[0].strip().upper()\n",
    "            if cod_secuencia in transcript_dict:\n",
    "                transcript_dict[cod_secuencia][\"fickett_score\"] = float(row[3])\n",
    "                transcript_dict[cod_secuencia][\"hexamer_score\"] = float(row[4])\n",
    "    \n",
    "    return transcript_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./libs/util_modelo_referencial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./libs/util_modelo_referencial.py\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import util_caracteristicas, util_fasta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals.joblib import dump, load\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "class GeneradorFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, identificador=None, X_train=None,y_train=None):\n",
    "        #generar todos los features\n",
    "        self.identificador = identificador\n",
    "        if X_train is None or y_train is None:\n",
    "            return\n",
    "        codigos_lncRNA = {}\n",
    "        codigos_PCT = {}\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i] == 0:\n",
    "                codigos_PCT[X_train[i][0]] = X_train[i][1]\n",
    "            else:\n",
    "                codigos_lncRNA[X_train[i][0]] = X_train[i][1]\n",
    "        util_caracteristicas.generar_modelo_CPAT(identificador, codigos_lncRNA, codigos_PCT)\n",
    "        util_caracteristicas.generar_caracteristicas(identificador, {**codigos_lncRNA, **codigos_PCT})\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #ejecutar cpat para el conjunto de datos\n",
    "        identificador = self.identificador\n",
    "        self._id_cpat = identificador + \"_fold_\" + hashlib.sha224(''.join([x[0] for x in X]).encode()).hexdigest()#str(hash(frozenset({x[0]:1 for x in X})))\n",
    "        id_cpat = self._id_cpat\n",
    "        if util_caracteristicas.existe_modelo_cpat(id_cpat):\n",
    "            return self\n",
    "        codigos_lncRNA = {}\n",
    "        codigos_PCT = {}\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 0:\n",
    "                codigos_PCT[X[i][0]] = X[i][1]\n",
    "            else:\n",
    "                codigos_lncRNA[X[i][0]] = X[i][1]\n",
    "        util_caracteristicas.generar_modelo_CPAT(id_cpat, codigos_lncRNA, codigos_PCT)\n",
    "        util_caracteristicas.generar_caracteristicas_cpat(identificador, {**codigos_lncRNA, **codigos_PCT})\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        #obtener caracteristicas\n",
    "        identificador = self.identificador\n",
    "        id_cpat = self._id_cpat\n",
    "        codigos = {codigo[0]:codigo[1] for codigo in X}\n",
    "        util_caracteristicas.generar_caracteristicas_cpat(id_cpat, codigos)\n",
    "        dict_features = util_caracteristicas.obtener_caracteristicas(identificador, id_cpat, codigos)\n",
    "\n",
    "        return [list(x.values()) for x in dict_features.values()]\n",
    "\n",
    "def crear_modelo_referencial(identificador, tuned_parameters, scores, n_jobs, cv):\n",
    "    if not os.path.isdir(\"./modelos_referenciales\"):\n",
    "        os.mkdir(\"./modelos_referenciales\")\n",
    "    \n",
    "    codigos_lncRNA = util_fasta.leer_fasta(\"./data/\" + identificador + \".lncRNA.fasta\")\n",
    "    codigos_PCT = util_fasta.leer_fasta(\"./data/\" + identificador + \".PCT.fasta\")\n",
    "    \n",
    "    X = list(codigos_lncRNA.items()) + list(codigos_PCT.items())\n",
    "    y = ([1] * len(codigos_lncRNA)) + ([0] * len(codigos_PCT))\n",
    "    X_train, y_train = X, y #shuffle(X, y, random_state=0)\n",
    "    svm_pipeline = Pipeline(steps=[('features', GeneradorFeatures(identificador, X_train, y_train)), ('svc', SVC())])\n",
    "    \n",
    "    for score in scores:\n",
    "        clf = GridSearchCV(svm_pipeline, tuned_parameters, cv=cv, scoring=score, n_jobs=n_jobs, refit=\"accuracy\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        resultado = {\n",
    "            \"accuracy\" : clf.cv_results_['mean_test_accuracy'][clf.best_index_],\n",
    "            \"precision\" : clf.cv_results_['mean_test_precision'][clf.best_index_],\n",
    "            \"recall\" : clf.cv_results_['mean_test_recall'][clf.best_index_]\n",
    "        }\n",
    "        dump(resultado, './modelos_referenciales/resultado_{}.bin'.format(identificador))\n",
    "        dump(clf.best_estimator_, './modelos_referenciales/modelo_{}.plk'.format(identificador), compress = 1)\n",
    "        \n",
    "        #means = clf.cv_results_['mean_test_accuracy']\n",
    "        #stds = clf.cv_results_['std_test_accuracy']\n",
    "        #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        #          % (mean, std * 2, params))\n",
    "        #print()\n",
    "\n",
    "        #print(\"Detailed classification report:\")\n",
    "        #print()\n",
    "        #print(\"The model is trained on the full development set.\")\n",
    "        #print(\"The scores are computed on the full evaluation set.\")\n",
    "        #print()\n",
    "        #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        #print(classification_report(y_true, y_pred))\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./libs/util_modelo_final.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./libs/util_modelo_final.py\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import util_caracteristicas, util_fasta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals.joblib import dump, load\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "def generar_features_generales(X_train, y_train):\n",
    "    #generar todos los features\n",
    "    identificador = \"final\"\n",
    "    codigos_lncRNA = {}\n",
    "    codigos_PCT = {}\n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i] == 0:\n",
    "            codigos_PCT[X_train[i][0]] = X_train[i][1]\n",
    "        else:\n",
    "            codigos_lncRNA[X_train[i][0]] = X_train[i][1]\n",
    "    util_caracteristicas.generar_modelo_CPAT(identificador, codigos_lncRNA, codigos_PCT)\n",
    "    util_caracteristicas.generar_caracteristicas(identificador, {**codigos_lncRNA, **codigos_PCT})\n",
    "\n",
    "def generar_fit(X, y):\n",
    "    #ejecutar cpat para el conjunto de datos\n",
    "    identificador = \"final\"\n",
    "    id_cpat = identificador + \"_fold_\" + hashlib.sha224(''.join([x[0] for x in X]).encode()).hexdigest()\n",
    "    codigos_lncRNA = {}\n",
    "    codigos_PCT = {}\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            codigos_PCT[X[i][0]] = X[i][1]\n",
    "        else:\n",
    "            codigos_lncRNA[X[i][0]] = X[i][1]\n",
    "    util_caracteristicas.generar_modelo_CPAT(id_cpat, codigos_lncRNA, codigos_PCT)\n",
    "    util_caracteristicas.generar_caracteristicas_cpat(identificador, {**codigos_lncRNA, **codigos_PCT})\n",
    "    \n",
    "def obtener_folds(ps, X_train, y_train):\n",
    "    for train_index, test_index in ps.split():\n",
    "        yield [X_train[x] for x in train_index], [y_train[y] for y in train_index]\n",
    "\n",
    "class GeneradorFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, identificador=None, folds_completos=None):\n",
    "        if identificador is None:\n",
    "            return\n",
    "        self.identificador = identificador\n",
    "        self.folds_completos = folds_completos\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #ejecutar cpat para el conjunto de datos\n",
    "        identificador = self.identificador\n",
    "        self._id_cpat = identificador + \"_fold_\" + hashlib.sha224(''.join([x[0] for x in X]).encode()).hexdigest()\n",
    "        if self._id_cpat == identificador + \"_fold_\" + self.folds_completos:\n",
    "            self._id_cpat = identificador\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        #obtener caracteristicas\n",
    "        identificador = self.identificador\n",
    "        id_cpat = self._id_cpat\n",
    "        codigos = {codigo[0]:codigo[1] for codigo in X}\n",
    "        util_caracteristicas.generar_caracteristicas_cpat(id_cpat, codigos)\n",
    "        dict_features = util_caracteristicas.obtener_caracteristicas(identificador, id_cpat, codigos)\n",
    "        return [list(x.values()) for x in dict_features.values()]\n",
    "\n",
    "def generar_sub_modelos(especie, folds, modelo):\n",
    "    X_train = folds[especie][\"X_train\"]\n",
    "    y_train = folds[especie][\"y_train\"]\n",
    "    modelo.fit(X_train, y_train)\n",
    "    dump(modelo, './modelo_final/modelo_{}.plk'.format(especie))\n",
    "    \n",
    "def crear_modelo_final(especies, tuned_parameters, scores, n_jobs):\n",
    "    print(\"Iniciando creación del modelo final\")\n",
    "    if not os.path.isdir(\"./modelo_final\"):\n",
    "        os.mkdir(\"./modelo_final\")\n",
    "    \n",
    "    X = list()\n",
    "    y = list()\n",
    "    test_fold = list()\n",
    "    indice_fold = 0\n",
    "    folds = {}\n",
    "    \n",
    "    for especie in especies:\n",
    "        folds[especie] = { \"llaves\" : list(), \"X_train\" : list(), \"y_train\" : list() }\n",
    "        \n",
    "    for especie in especies:\n",
    "        identificador = especie\n",
    "        codigos_lncRNA = util_fasta.leer_fasta(\"./data/\" + identificador + \".lncRNA.fasta\")\n",
    "        codigos_PCT = util_fasta.leer_fasta(\"./data/\" + identificador + \".PCT.fasta\")\n",
    "        \n",
    "        _x = list(codigos_lncRNA.items()) + list(codigos_PCT.items())\n",
    "        _y = ([1] * len(codigos_lncRNA)) + ([0] * len(codigos_PCT))\n",
    "        X = X + _x\n",
    "        y = y + _y\n",
    "        test_fold = test_fold + ([indice_fold] * (len(codigos_lncRNA) + len(codigos_PCT)))\n",
    "        \n",
    "        folds[especie][\"indice\"] = indice_fold\n",
    "        for especie_2 in especies:\n",
    "            if especie != especie_2:\n",
    "                folds[especie_2][\"llaves\"] = folds[especie_2][\"llaves\"] + [item[0] for item in _x]\n",
    "                folds[especie_2][\"X_train\"] = folds[especie_2][\"X_train\"] + _x\n",
    "                folds[especie_2][\"y_train\"] = folds[especie_2][\"y_train\"] + _y\n",
    "        \n",
    "        indice_fold = indice_fold + 1\n",
    "        \n",
    "    folds_especies = {}\n",
    "    for especie in especies:\n",
    "        folds[especie][\"llave_hash\"] = \"final_fold_\" + hashlib.sha224(''.join(folds[especie][\"llaves\"]).encode()).hexdigest()\n",
    "        folds_especies[folds[especie][\"llave_hash\"]] = {\n",
    "            \"especie\" : especie,\n",
    "            \"fold\" : folds[especie][\"indice\"],\n",
    "            \"num_transcritos\" : len(folds[especie][\"llaves\"]),\n",
    "            \"X_train\" : folds[especie][\"X_train\"], \n",
    "            \"y_train\" : folds[especie][\"y_train\"]\n",
    "        }\n",
    "    dump(folds_especies, './modelo_final/folds_especies.bin')\n",
    "        \n",
    "    X_train, y_train = X, y #shuffle(X, y, random_state=0)\n",
    "    identificador = \"final\"\n",
    "    folds_completos = hashlib.sha224(''.join([x[0] for x in X_train]).encode()).hexdigest()\n",
    "    svm_pipeline = Pipeline(steps=[('features', GeneradorFeatures(identificador, folds_completos)), ('svc', SVC())])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    generar_features_generales(X_train, y_train)\n",
    "    print(\"Prepocesamiento de transcritos previos al entrenamiento del modelo final\")\n",
    "    #Parallel(n_jobs=n_jobs, verbose=0)(delayed(generar_fit)(X, y) for X, y in obtener_folds(ps, X_train, y_train))\n",
    "    for _X_train, _y_train in obtener_folds(ps, X_train, y_train):\n",
    "        generar_fit(_X_train, _y_train)\n",
    "    \n",
    "    for score in scores:\n",
    "        print(\"Entrenamiento del modelo con grid search\")\n",
    "        clf = GridSearchCV(svm_pipeline, tuned_parameters, cv=ps, scoring=score, n_jobs=n_jobs, refit=\"accuracy\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        resultado = {\n",
    "            \"accuracy\" : clf.cv_results_['mean_test_accuracy'][clf.best_index_],\n",
    "            \"precision\" : clf.cv_results_['mean_test_precision'][clf.best_index_],\n",
    "            \"recall\" : clf.cv_results_['mean_test_recall'][clf.best_index_]\n",
    "        }\n",
    "        dump(resultado, './modelo_final/resultado_{}.bin'.format(identificador))\n",
    "        dump(clf.best_params_, './modelo_final/params_{}.bin'.format(identificador))\n",
    "        dump(clf.cv_results_, './modelo_final/cv_results_{}.bin'.format(identificador))\n",
    "        dump(clf.best_estimator_, './modelo_final/modelo_{}.plk'.format(identificador))\n",
    "        \n",
    "        print(\"Generación de modelos finales dejando afuera una especie\")\n",
    "        Parallel(n_jobs=n_jobs, verbose=0)(delayed(generar_sub_modelos)(especie, folds, clf.best_estimator_) for especie in especies)\n",
    "        \n",
    "        #means = clf.cv_results_['mean_test_accuracy']\n",
    "        #stds = clf.cv_results_['std_test_accuracy']\n",
    "        #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "        #          % (mean, std * 2, params))\n",
    "        #print()\n",
    "\n",
    "        #print(\"Detailed classification report:\")\n",
    "        #print()\n",
    "        #print(\"The model is trained on the full development set.\")\n",
    "        #print(\"The scores are computed on the full evaluation set.\")\n",
    "        #print()\n",
    "        #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        #print(classification_report(y_true, y_pred))\n",
    "        #print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
