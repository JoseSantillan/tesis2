{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revisaron pappers para investigar los features que se suelen usar en el estado del arte.<br>\n",
    "Los pappers seleccionados fueron los siguientes:<br>\n",
    "<ul>\n",
    "    <li>Prediction of plant lncRNA by ensemble machine learning classifiers</li>\n",
    "    <li>LncADeep- an ab initio lncRNA identification and functional annotation tool based on deep learning</li>\n",
    "    <li>A deep recurrent neural network discovers complex biological rules to decipher RNA protein-coding potential</li>\n",
    "    <li>A Support Vector Machine based method to distinguish long non-coding RNAs from protein coding transcripts</li>\n",
    "</ul>\n",
    "<h1>1.Prediction of plant lncRNA by ensemble machine learning classifiers</h1>\n",
    "Los features utilizados en este papper son:\n",
    "<ol>\n",
    "    <li>Longitud del transcriptoma</li>\n",
    "    <li>Longitud del open reading frame (ORF)</li>\n",
    "    <li>Porcentaje de nucleótidos GC presentes</li>\n",
    "    <li>Fickett score</li>\n",
    "    <li>Hexamer score</li>\n",
    "    <li>Alignment identity in SwissProt database</li>\n",
    "    <li>Longitud del alineamiento en la base de datos SwissProt</li>\n",
    "    <li>Proporción de la longitud del alineamiento con respecto a la longitud del transcriptoma</li>\n",
    "    <li>Proporción de la longitud del alineamiento y la longitud del ORF</li>\n",
    "    <li>Pesencia de elementos transponibles (pero descartado en el papper, debido a que toma mucho tiempo calcular y no aportar mucho en términos del proceso de clasificación)</li>\n",
    "    <li>Divergencia del transcriptoma del elemento transponible (pero descartado en el papper, debido a que toma mucho tiempo calcular y no aportar mucho en términos del proceso de clasificación)</li>\n",
    "</ol>\n",
    "<h2>1.1.Longitud del transcriptoma</h2>\n",
    "Este feature ya se calculó.\n",
    "<h2>1.2.Longitud del open reading frame (ORF)</h2>\n",
    "Para calcular este feature, se utilizó en el paper el programa CPAT.<br/>\n",
    "<h3>1.2.1.CPAT</h3>\n",
    "Se procede con la instalación de programa.\n",
    "<h4>1.2.1.1.Instalación de CPAT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cpat\n",
      "  Using cached https://files.pythonhosted.org/packages/52/4e/63b692096ba9b3f4b399897744b88685dd68be01ff8f341f5c59fd514e34/CPAT-2.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /home/jose/anaconda3/lib/python3.7/site-packages (from cpat) (1.16.2)\n",
      "Requirement already satisfied: pysam in /home/jose/anaconda3/lib/python3.7/site-packages (from cpat) (0.15.2)\n",
      "Installing collected packages: cpat\n",
      "Successfully installed cpat-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install cpat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado, existe otro requisito utilizado como input por CPAT...\n",
    "<h4>1.2.1.2.Generación del las tablas de frame hexamer (6mer) frequency</h4>\n",
    "De acuerdo a <a href=\"http://rna-cpat.sourceforge.net/\">http://rna-cpat.sourceforge.net/</a> para generar lo anterior, se requieren CDS de los ARN codificantes, y secuencias no codificantes. Lo segundo ya se posee, pero de lo primero solo se tienen los transcriptomas completos. Ensembl permite descargar las secuencias codificantes de estos archivos. Se procederá con su descarga.<br/>\n",
    "<h5>1.2.1.2.1.Descarga de los CDS se Ensembl</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iniciando proceso...\n",
       "\r",
       "Procesando especie #1 Amborella trichopoda                                      \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 1000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 2000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 3000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 4000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 5000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 6000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 7000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 8000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 9000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 10000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 11000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 12000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 13000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 14000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 15000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 16000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 17000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 18000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 19000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 20000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 21000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 22000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 23000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 24000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 25000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 26000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 27000 filas                     \r",
       "Procesando especie #1 Amborella trichopoda - procesadas 27313 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata                                      \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 1000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 2000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 3000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 4000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 5000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 6000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 7000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 8000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 9000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 10000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 11000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 12000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 13000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 14000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 15000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 16000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 17000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 18000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 19000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 20000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 21000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 22000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 23000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 24000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 25000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 26000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 27000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 28000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 29000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 30000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 31000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 32000 filas                     \r",
       "Procesando especie #2 Arabidopsis lyrata - procesadas 32667 filas                     \r",
       "Procesando especie #3 Arabidopsis thaliana                                      \r",
       "Error al procesar la especie Arabidopsis thaliana                                           \n",
       "\r",
       "Procesando especie #4 Brachypodium distachyon                                      \r",
       "Error al procesar la especie Brachypodium distachyon                                           \n",
       "\r",
       "Procesando especie #5 Brassica napus                                      \r",
       "Error al procesar la especie Brassica napus                                           \n",
       "\r",
       "Procesando especie #6 Brassica oleracea                                      \r",
       "Error al procesar la especie Brassica oleracea                                           \n",
       "\r",
       "Procesando especie #7 Brassica rapa                                      \r",
       "Error al procesar la especie Brassica rapa                                           \n",
       "\r",
       "Procesando especie #8 Corchorus capsularis                                      \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 1000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 2000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 3000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 4000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 5000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 6000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 7000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 8000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 9000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 10000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 11000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 12000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 13000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 14000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 15000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 16000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 17000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 18000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 19000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 20000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 21000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 22000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 23000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 24000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 25000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 26000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 27000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 28000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 29000 filas                     \r",
       "Procesando especie #8 Corchorus capsularis - procesadas 29356 filas                     \r",
       "Procesando especie #9 Cucumis sativus                                      \r",
       "Procesando especie #9 Cucumis sativus - procesadas 1000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 2000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 3000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 4000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 5000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 6000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 7000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 8000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 9000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 10000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 11000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 12000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 13000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 14000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 15000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 16000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 17000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 18000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 19000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 20000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 21000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 22000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 23000 filas                     \r",
       "Procesando especie #9 Cucumis sativus - procesadas 23780 filas                     \r",
       "Procesando especie #10 Glycine max                                      \r",
       "Error al procesar la especie Glycine max                                           \n",
       "\r",
       "Procesando especie #11 Gossypium raimondii                                      \r",
       "Error al procesar la especie Gossypium raimondii                                           \n",
       "\r",
       "Procesando especie #12 Hordeum vulgare                                      \r",
       "Error al procesar la especie Hordeum vulgare                                           \n",
       "\r",
       "Procesando especie #13 Leersia perrieri                                      \r",
       "Error al procesar la especie Leersia perrieri                                           \n",
       "\r",
       "Procesando especie #14 Manihot esculenta                                      \r",
       "Error al procesar la especie Manihot esculenta                                           \n",
       "\r",
       "Procesando especie #15 Medicago truncatula                                      \r",
       "Error al procesar la especie Medicago truncatula                                           \n",
       "\r",
       "Procesando especie #16 Oryza barthii                                      \r",
       "Error al procesar la especie Oryza barthii                                           \n",
       "\r",
       "Procesando especie #17 Oryza brachyantha                                      \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 1000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 2000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 3000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 4000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 5000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 6000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 7000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 8000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 9000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 10000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 11000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 12000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 13000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 14000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 15000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 16000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 17000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 18000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 19000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 20000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 21000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 22000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 23000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 24000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 25000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 26000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 27000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 28000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 29000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 30000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 31000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 32000 filas                     \r",
       "Procesando especie #17 Oryza brachyantha - procesadas 32037 filas                     \r",
       "Procesando especie #18 Oryza nivara                                      \r",
       "Error al procesar la especie Oryza nivara                                           \n",
       "\r",
       "Procesando especie #19 Oryza rufipogon                                      \r",
       "Error al procesar la especie Oryza rufipogon                                           \n",
       "\r",
       "Procesando especie #20 Oryza sativa Japonica Group                                      \r",
       "Error al procesar la especie Oryza sativa Japonica Group                                           \n",
       "\r",
       "Procesando especie #21 Physcomitrella patens                                      \r",
       "Error al procesar la especie Physcomitrella patens                                           \n",
       "\r",
       "Procesando especie #22 Populus trichocarpa                                      \r",
       "Error al procesar la especie Populus trichocarpa                                           \n",
       "\r",
       "Procesando especie #23 Setaria italica                                      \r",
       "Procesando especie #23 Setaria italica - procesadas 1000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 2000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 3000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 4000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 5000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 6000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 7000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 8000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 9000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 10000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 11000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 12000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 13000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 14000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 15000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 16000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 17000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 18000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 19000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 20000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 21000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 22000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 23000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 24000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 25000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 26000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 27000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 28000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 29000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 30000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 31000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 32000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 33000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 34000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 35000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 36000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 37000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 38000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 39000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 40000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 41000 filas                     \r",
       "Procesando especie #23 Setaria italica - procesadas 41023 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum                                      \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 1000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 2000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 3000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 4000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 5000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 6000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 7000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 8000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 9000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 10000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 11000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 12000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 13000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 14000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 15000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 16000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 17000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 18000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 19000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 20000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 21000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 22000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 23000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 24000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 25000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 26000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 27000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 28000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 29000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 30000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 31000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 32000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 33000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 34000 filas                     \r",
       "Procesando especie #24 Solanum lycopersicum - procesadas 34429 filas                     \r",
       "Procesando especie #25 Solanum tuberosum                                      \r",
       "Error al procesar la especie Solanum tuberosum                                           \n",
       "\r",
       "Procesando especie #26 Sorghum bicolor                                      \r",
       "Error al procesar la especie Sorghum bicolor                                           \n",
       "\r",
       "Procesando especie #27 Theobroma cacao                                      \r",
       "Error al procesar la especie Theobroma cacao                                           \n",
       "\r",
       "Procesando especie #28 Trifolium pratense                                      \r",
       "Procesando especie #28 Trifolium pratense - procesadas 1000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 2000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 3000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 4000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 5000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 6000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 7000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 8000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 9000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 10000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 11000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 12000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 13000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 14000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 15000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 16000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 17000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 18000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 19000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 20000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 21000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 22000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 23000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 24000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 25000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 26000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 27000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 28000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 29000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 30000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 31000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 32000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 33000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 34000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 35000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 36000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 37000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 38000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 39000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 40000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 41000 filas                     \r",
       "Procesando especie #28 Trifolium pratense - procesadas 41270 filas                     \r",
       "Procesando especie #29 Triticum aestivum                                      \r",
       "Error al procesar la especie Triticum aestivum                                           \n",
       "\r",
       "Procesando especie #30 Vitis vinifera                                      \r",
       "Procesando especie #30 Vitis vinifera - procesadas 1000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 2000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 3000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 4000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 5000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 6000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 7000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 8000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 9000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 10000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 11000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 12000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 13000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 14000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 15000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 16000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 17000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 18000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 19000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 20000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 21000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 22000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 23000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 24000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 25000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 26000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 27000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 28000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 29000 filas                     \r",
       "Procesando especie #30 Vitis vinifera - procesadas 29927 filas                     \r",
       "Proceso finalizado...                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 42s, sys: 52.3 s, total: 3min 35s\n",
      "Wall time: 2h 24min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "library(RMySQL)\n",
    "library(curl)\n",
    "library(biomaRt)\n",
    "library(stringr)\n",
    "\n",
    "cat(\"Iniciando proceso...\\n\")\n",
    "\n",
    "dsn_database = \"tesis2\"\n",
    "dsn_hostname = \"127.0.0.1\"\n",
    "dsn_port = 3306\n",
    "dsn_uid = \"tesis2\"\n",
    "dsn_pwd = \"tesis2\"\n",
    "\n",
    "conn = dbConnect(MySQL(), user=dsn_uid, password=dsn_pwd, dbname=dsn_database, host=dsn_hostname)\n",
    "rs = dbSendQuery(conn, \"SELECT m.id_especie, m.especie FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\")\n",
    "especies = dbFetch(rs)\n",
    "dbClearResult(rs)\n",
    "\n",
    "ensembl_mart=useMart(\"plants_mart\", host=\"plants.ensembl.org\")\n",
    "lista_datasets = listDatasets(ensembl_mart)\n",
    "\n",
    "if (nrow(especies) > 0) {\n",
    "    for(i in 1:nrow(especies)) {\n",
    "        tryCatch(\n",
    "            expr = {\n",
    "                cat(\"\\rProcesando especie #\", i, \" \", especies[i,2], \"                                      \", sep = \"\")\n",
    "                la_especie = especies[i,2]\n",
    "                especie = paste(\"^\", la_especie, sep=\"\")\n",
    "                dataset = lista_datasets[which(str_detect(lista_datasets$description, especie)), 1]\n",
    "                ensembl_ds = useDataset(dataset,mart=ensembl_mart)\n",
    "                genes = getBM(attributes=c('ensembl_transcript_id', 'coding'), filters = 'transcript_biotype', values = 'protein_coding', mart = ensembl_ds)\n",
    "\n",
    "                for(j in 1:nrow(genes)) {\n",
    "                    if (j %% 1000 == 0) {\n",
    "                        cat(\"\\rProcesando especie #\", i, \" \", especies[i,2], \" - procesadas \", j, \" filas                     \", sep = \"\")\n",
    "                    }\n",
    "                    if (!(genes[j,2] == \"Sequence unavailable\" | str_length(genes[j,2]) > 65000)) {\n",
    "                        query = sprintf(\"INSERT INTO secuencias_CDS (id_especie, cod_secuencia, coding) VALUES (%d, '%s', '%s')\", especies[i,1], genes[j,1], genes[j,2])\n",
    "                        dbSendQuery(conn, query)\n",
    "                    }\n",
    "                }\n",
    "                cat(\"\\rProcesando especie #\", i, \" \", especies[i,2], \" - procesadas \", nrow(genes), \" filas                     \", sep = \"\")\n",
    "            },\n",
    "            error = function(e) {\n",
    "                cat(\"\\rError al procesar la especie \", especies[i,2], \"                                           \\n\", sep = \"\")\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "dbDisconnect(conn)\n",
    "\n",
    "cat(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.2.1.2.2.Procesamiento de archivos descargados manualmente porque fallaron en la descarga automática</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 2min 6s, sys: 27.2 s, total: 2min 33s\n",
      "Wall time: 5min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "#método para leer los transcriptomas de un archivo fasta de ensembl\n",
    "def leer_transcriptomas_ensembl(archivo, cur, id_especie):\n",
    "    cabecera = \"\"\n",
    "    transcriptoma = \"\"\n",
    "    f = open(archivo, \"r\")\n",
    "    for linea in f:\n",
    "        if linea.startswith(\">\"):\n",
    "            if transcriptoma != \"\":\n",
    "                if transcriptoma != \"Sequence unavailable\" and len(transcriptoma) <= 65000:\n",
    "                    query = \"INSERT INTO secuencias_CDS (id_especie, cod_secuencia, coding) VALUES (%s, %s, %s)\"\n",
    "                    cur.execute(query, (id_especie, cabecera, transcriptoma))\n",
    "                transcriptoma = \"\"\n",
    "            cabecera = linea.rstrip(\"\\n\").lstrip(\">\")\n",
    "        else:\n",
    "            transcriptoma += linea.rstrip(\"\\n\")\n",
    "    if transcriptoma != \"\" and len(transcriptoma) <= 65000:\n",
    "        if transcriptoma != \"Sequence unavailable\":\n",
    "            query = \"INSERT INTO secuencias_CDS (id_especie, cod_secuencia, coding) VALUES (%s, %s, %s)\"\n",
    "            cur.execute(query, (id_especie, cabecera, transcriptoma))\n",
    "        transcriptoma = \"\"\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "directory = '../Semana 05/descarga_manual_ensembl_CDS'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".fasta\"):\n",
    "        especie = os.path.splitext(filename)[0]\n",
    "        query = \"SELECT COUNT(*) FROM maestra_especies m WHERE especie IN (SELECT c1.especie FROM conteo_lncrna_especie c1 LEFT JOIN conteo_lncrna_especie c2 ON c1.especie = REPLACE(c2.especie, '_', ' ') AND c1.fuente = 'Ensembl' AND c2.fuente = 'CantataDB' LEFT JOIN conteo_lncrna_especie c3 ON c1.especie = REPLACE(c3.especie, '_', ' ') AND c1.fuente = 'Ensembl' AND c3.fuente = 'GreeNC' WHERE c1.fuente = 'Ensembl' AND (c2.fuente IS NOT NULL OR c3.fuente IS NOT NULL)) AND m.especie = REPLACE(%s, '_', ' ')\"\n",
    "        cur.execute(query, [especie])\n",
    "        result=cur.fetchone()\n",
    "        if result[0] > 0:\n",
    "            print(\"\\rProcesando especie \", especie, \"                                        \", end=\" \")\n",
    "            query = \"SELECT e.id_especie FROM maestra_especies e WHERE e.especie = REPLACE(%s, '_', ' ')\"\n",
    "            cur.execute(query, [especie])\n",
    "            id_especie=cur.fetchone()\n",
    "            query = \"delete from secuencias_CDS where id_especie = %s\"\n",
    "            cur.execute(query, [id_especie[0]])\n",
    "            leer_transcriptomas_ensembl(directory + \"/\" + filename, cur, id_especie[0])\n",
    "            conn.commit()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.2.1.2.3.Generación de archivos fasta con las secuencias CDS de los ARN codificantes, y los transcriptomas de los ARN no codificantes</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 7.48 s, sys: 708 ms, total: 8.19 s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "def generar_fasta(secuencias, especie, tipo, directory):\n",
    "    t_tamanio = 80\n",
    "    f = open(directory + especie + \".\" + tipo + \".fasta\" ,\"w+\")\n",
    "    for transcriptoma in secuencias:\n",
    "        f.write(\">%s\\n\" % (transcriptoma[0]))\n",
    "        seq = transcriptoma[1]\n",
    "        t_partes = [seq[i:i+t_tamanio] for i in range(0, len(seq), t_tamanio)]\n",
    "        for t_parte in t_partes:\n",
    "            f.write(\"%s\\n\" % (t_parte))\n",
    "    f.close()\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "directory = '../Semana 05/fasta_para_hexamer/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando fasta CDS para \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    query = \"select c.cod_secuencia, c.coding from secuencias_CDS c JOIN secuencias_features f ON c.id_especie = f.id_especie AND c.cod_secuencia = f.cod_secuencia WHERE c.id_especie = %s AND f.flg_pct = 1 AND f.flg_seleccionado = 1\"\n",
    "    cursor.execute(query, [especie[0]])\n",
    "    secuencias = cursor.fetchall()\n",
    "    generar_fasta(secuencias, especie[1], \"CDS\", directory)\n",
    "    print(\"\\rGenerando fasta lncRNA para \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    query = \"SELECT cod_secuencia, secuencia FROM secuencias WHERE id_especie = %s AND flg_pct = 0 AND flg_seleccionado = 1\"\n",
    "    cursor.execute(query, [especie[0]])\n",
    "    secuencias = cursor.fetchall()\n",
    "    generar_fasta(secuencias, especie[1], \"lncRNA\", directory)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.2.1.2.4.Generación de la tabla hexamer</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 1min 11s, sys: 0 ns, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "input_directory = '../Semana 05/fasta_para_hexamer/'\n",
    "output_directory = '../Semana 05/tablas_hexamer/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando tabla hexamer para \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    script = \"~/anaconda3/bin/make_hexamer_tab.py\"\n",
    "    fasta_cds = \"'\" + input_directory + especie[1] + \".CDS.fasta\" + \"'\" \n",
    "    lncRNA_cds = \"'\" + input_directory + especie[1] + \".lncRNA.fasta\" + \"'\"\n",
    "    salida = output_directory + especie[1] + \".tsv\"\n",
    "    with io.capture_output() as captured:\n",
    "        %run $script -c $fasta_cds -n $lncRNA_cds\n",
    "        f = open(salida ,\"w+\")\n",
    "        f.write(captured.stdout)\n",
    "        f.close()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un segundo requisito para ejecutar CPAT, será generar el archivo de regresión logística.\n",
    "<h4>1.2.1.3.Generación del archivo de regresión logística</h4>\n",
    "Este archivo requiere los transcriptomas de los ARN codificantes. Aún no se han generado archivos fasta para ellos.\n",
    "<h5>1.2.1.3.1.Generación de archivos fasta de transcriptomas de ARN codificantes</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 10.1 s, sys: 1.32 s, total: 11.4 s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "\n",
    "def generar_fasta(secuencias, especie, tipo, directory):\n",
    "    t_tamanio = 80\n",
    "    f = open(directory + especie + \".\" + tipo + \".fasta\" ,\"w+\")\n",
    "    for transcriptoma in secuencias:\n",
    "        f.write(\">%s\\n\" % (transcriptoma[0]))\n",
    "        seq = transcriptoma[1]\n",
    "        t_partes = [seq[i:i+t_tamanio] for i in range(0, len(seq), t_tamanio)]\n",
    "        for t_parte in t_partes:\n",
    "            f.write(\"%s\\n\" % (t_parte))\n",
    "    f.close()\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "directory = '../Semana 05/fasta_pct_para_logit/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando fasta CDS para \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    query = \"SELECT cod_secuencia, secuencia FROM secuencias WHERE id_especie = %s AND flg_pct = 1 AND flg_seleccionado = 1\"\n",
    "    cursor.execute(query, [especie[0]])\n",
    "    secuencias = cursor.fetchall()\n",
    "    generar_fasta(secuencias, especie[1], \"PCT\", directory)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.2.1.3.2.Generación del archivo de regresión logística</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 7min 13s, sys: 1.4 s, total: 7min 14s\n",
      "Wall time: 7min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "input_directory = '../Semana 05/tablas_hexamer/'\n",
    "input_directory_pct = '../Semana 05/fasta_pct_para_logit/'\n",
    "input_directory_lncRNA = '../Semana 05/fasta_para_hexamer/'\n",
    "output_directory_logit = './modelo_regresion_logistica/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando modelo de regresión para \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    script = \"~/anaconda3/bin/make_logitModel.py\"\n",
    "    archivo_hexamer = \"'\" + input_directory + especie[1] + \".tsv\" + \"'\"\n",
    "    fasta_pct = \"'\" + input_directory_pct + especie[1] + \".PCT.fasta\" + \"'\"\n",
    "    lncRNA_cds = \"'\" + input_directory_lncRNA + especie[1] + \".lncRNA.fasta\" + \"'\"\n",
    "    salida = \"'\" + output_directory_logit + especie[1] + \"'\"\n",
    "    with io.capture_output() as captured:\n",
    "        %run $script -x $archivo_hexamer -c $fasta_pct -n $lncRNA_cds -o $salida\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.2.1.2.Ejecución de CPAT</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 7min 47s, sys: 1.5 s, total: 7min 49s\n",
      "Wall time: 8min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "input_directory = '../Semana 05/tablas_hexamer/'\n",
    "input_directory_pct = '../Semana 05/fasta_pct_para_logit/'\n",
    "input_directory_lncRNA = '../Semana 05/fasta_para_hexamer/'\n",
    "input_directory_logit = '../Semana 05/modelo_regresion_logistica/'\n",
    "output_directory = './cpat_features/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando features cpat para PTC \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    script = \"~/anaconda3/bin/cpat.py\"\n",
    "    fasta_pct = \"'\" + input_directory_pct + especie[1] + \".PCT.fasta\" + \"'\"\n",
    "    archivo_logit = \"'\" + input_directory_logit + especie[1] + \".logit.RData\" + \"'\"\n",
    "    archivo_hexamer = \"'\" + input_directory + especie[1] + \".tsv\" + \"'\"\n",
    "    salida = \"'\" + output_directory + especie[1] + \".PCT\" + \"'\"\n",
    "    with io.capture_output() as captured:\n",
    "        %run $script -g $fasta_pct -d $archivo_logit -x $archivo_hexamer -o $salida\n",
    "    print(\"\\rGenerando features cpat para lncRNA \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    fasta_lncRNA = \"'\" + input_directory_lncRNA + especie[1] + \".lncRNA.fasta\" + \"'\"\n",
    "    salida = \"'\" + output_directory + especie[1] + \".lncRNA\" + \"'\"\n",
    "    with io.capture_output() as captured:\n",
    "        %run $script -g $fasta_lncRNA -d $archivo_logit -x $archivo_hexamer -o $salida\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se tienen archivo de salida de CPAT con el ORF (y otros features de interés).<br/>\n",
    "Se procede a la importación de estos features a la base de datos.<br/>\n",
    "<h4>1.2.1.3.Importación del ORF y otros features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def procesar_features_cpat(archivo, cur, id_especie):\n",
    "    with open(archivo, \"r\") as tabular:\n",
    "        cpat_reader = csv.reader(tabular, delimiter=(\"\\t\"))\n",
    "        for row in cpat_reader:\n",
    "            cod_secuencia = row[0]\n",
    "            orf_length = float(row[2])\n",
    "            fickett = float(row[3])\n",
    "            hexamer = float(row[4])\n",
    "            query = \"UPDATE secuencias_features SET orf_length = %s, orf_coverage = %s/longitud, ficket_score = %s, hexamer_score = %s WHERE id_especie = %s and cod_secuencia = %s\"\n",
    "            cur.execute(query, (orf_length, orf_length, fickett, hexamer, id_especie, cod_secuencia))\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "directory = '../Semana 05/cpat_features'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".dat\"):\n",
    "        especie = filename.split(\".\")[0]\n",
    "        query = \"SELECT COUNT(*) FROM maestra_especies m WHERE especie IN (SELECT c1.especie FROM conteo_lncrna_especie c1 LEFT JOIN conteo_lncrna_especie c2 ON c1.especie = REPLACE(c2.especie, '_', ' ') AND c1.fuente = 'Ensembl' AND c2.fuente = 'CantataDB' LEFT JOIN conteo_lncrna_especie c3 ON c1.especie = REPLACE(c3.especie, '_', ' ') AND c1.fuente = 'Ensembl' AND c3.fuente = 'GreeNC' WHERE c1.fuente = 'Ensembl' AND (c2.fuente IS NOT NULL OR c3.fuente IS NOT NULL)) AND m.especie = REPLACE(%s, '_', ' ')\"\n",
    "        cur.execute(query, [especie])\n",
    "        result=cur.fetchone()\n",
    "        if result[0] > 0:\n",
    "            print(\"\\rProcesando especie\", especie, filename.split(\".\")[1],\"                         \", end=\" \")\n",
    "            query = \"SELECT e.id_especie FROM maestra_especies e WHERE e.especie = REPLACE(%s, '_', ' ')\"\n",
    "            cur.execute(query, [especie])\n",
    "            id_especie=cur.fetchone()\n",
    "            procesar_features_cpat(directory + \"/\" + filename, cur, id_especie[0])\n",
    "            conn.commit()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3.Porcentaje de nucleótidos GC presentes</h2>\n",
    "El caĺculo se realiza a través de la librería Bio.SeqUtils.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/5d/8aedf28541f4936707d78997ebe6d9e25935ae6df6b8f7a045ce294df664/biopython-1.73-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 7.0MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jose/anaconda3/lib/python3.7/site-packages (from biopython) (1.16.2)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.73\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 21.6 s, sys: 5.04 s, total: 26.7 s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "from Bio.SeqUtils import GC\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rGenerando GC content \" + str(especie[0]), \"                                       \", end=\"\")\n",
    "    query = \"SELECT cod_secuencia, secuencia FROM secuencias WHERE id_especie = %s AND flg_seleccionado = 1\"\n",
    "    cursor.execute(query, [especie[0]])\n",
    "    transcriptomas = cursor.fetchall()\n",
    "    for transcriptoma in transcriptomas:\n",
    "        gc_content = GC(transcriptoma[1])\n",
    "        query = \"UPDATE secuencias_features SET gc_content = %s WHERE id_especie = %s and cod_secuencia = %s\"\n",
    "        cur.execute(query, (gc_content, especie[0], transcriptoma[0]))\n",
    "    conn.commit()\n",
    "    \n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.4.Fickett score</h2>\n",
    "El caĺculo ya ha sido realizado con CPAT.<br/>\n",
    "<h2>1.5.Hexamer score</h2>\n",
    "El caĺculo ya ha sido realizado con CPAT.<br/>\n",
    "<h2>1.6.Alignment identity in SwissProt database</h2>\n",
    "Para el cálculo de este feature se usó el algoritmo diamond para la búsqueda por alineamiento.\n",
    "<h3>1.6.1.Instalación de diamond</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conda.compat module is deprecated and will be removed in a future release.\n",
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/jose/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - diamond\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  diamond            bioconda/linux-64::diamond-0.9.24-ha87ae23_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} -c bioconda diamond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar diamond, se debe generar una base de datos de origen, en dónde se buscarán las coincidencias.\n",
    "<h3>1.6.2.Base de datos diamond</h3>\n",
    "Se descargó un archivo fasta con la información a ingresar a la base de datos, desde <a href=\"https://www.uniprot.org/uniprot/?query=taxonomy:viridiplantae&fil=reviewed%3Ayes&sort=score\">https://www.uniprot.org/uniprot/?query=taxonomy:viridiplantae&fil=reviewed%3Ayes&sort=score</a><br>\n",
    "Se filtraron los resultados para plantas, validadas experimentalmente.<br/>\n",
    "Con esto se procede a crear la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diamond v0.9.24.125 | by Benjamin Buchfink <buchfink@gmail.com>\n",
      "Licensed under the GNU GPL <https://www.gnu.org/licenses/gpl.txt>\n",
      "Check http://github.com/bbuchfink/diamond for updates.\n",
      "\n",
      "#CPU threads: 4\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Database file: ../Semana 05/diamond/uniprot-viridiplantae-reviewed.fasta\n",
      "Opening the database file...  [0.000237s]\n",
      "Loading sequences...  [0.115702s]\n",
      "Masking sequences...  [2.24115s]\n",
      "Writing sequences...  [0.040182s]\n",
      "Hashing sequences...  [0.009778s]\n",
      "Loading sequences...  [2.5e-05s]\n",
      "Writing trailer...  [0.001172s]\n",
      "Closing the input file...  [2.6e-05s]\n",
      "Closing the database file...  [3.8e-05s]\n",
      "Database hash = e3fb30fb072672cc3aab0b36faa2ef6d\n",
      "Processed 39800 sequences, 15258335 letters.\n",
      "Total time = 2.40875s\n",
      "CPU times: user 41.9 ms, sys: 16 ms, total: 57.8 ms\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "!~/anaconda3/bin/diamond makedb --in '../Semana 05/diamond/uniprot-viridiplantae-reviewed.fasta' -d '../Semana 05/diamond/uniprot-viridiplantae-reviewed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.6.3.Búsqueda de alineamientos con diamond</h3>\n",
    "Para la búsqueda se utilizará un comando com parámetros similares a los del papper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 13.8 s, sys: 2.25 s, total: 16.1 s\n",
      "Wall time: 51min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import mysql.connector\n",
    "import os\n",
    "from IPython.utils import io\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "input_directory_pct = '../Semana 05/fasta_pct_para_logit/'\n",
    "input_directory_lncRNA = '../Semana 05/fasta_para_hexamer/'\n",
    "output_directory = '../Semana 05/diamond_features/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rRealizando búsqueda para \" + str(especie[0]), \" PCT                                      \", end=\"\")\n",
    "    diamond_db = \"'\" + '../Semana 05/diamond/uniprot-viridiplantae-reviewed.dmnd' + \"'\"\n",
    "    fasta_pct = \"'\" + input_directory_pct + especie[1] + \".PCT.fasta\" + \"'\"\n",
    "    lncRNA_cds = \"'\" + input_directory_lncRNA + especie[1] + \".lncRNA.fasta\" + \"'\"\n",
    "    salida = \"'\" + output_directory + especie[1] + \".PCT.tsv\" + \"'\"\n",
    "    with io.capture_output() as captured:\n",
    "        !~/anaconda3/bin/diamond blastx -d $diamond_db -q $fasta_pct -o $salida -k 5 --gapopen 11 --gapextend 1 --more-sensitive -f 6 qseqid pident length qframe qstart qend sstart send evalue bitscore\n",
    "    print(\"\\rRealizando búsqueda para \" + str(especie[0]), \" lncRNA                                      \", end=\"\")\n",
    "    lncRNA_cds = \"'\" + input_directory_lncRNA + especie[1] + \".lncRNA.fasta\" + \"'\"\n",
    "    salida = \"'\" + output_directory + especie[1] + \".lncRNA.tsv\" + \"'\"\n",
    "    with io.capture_output() as captured:\n",
    "        !~/anaconda3/bin/diamond blastx -d $diamond_db -q $lncRNA_cds -o $salida -k 5 --gapopen 11 --gapextend 1 --more-sensitive -f 6 qseqid pident length qframe qstart qend sstart send evalue bitscore\n",
    "    \n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.6.4.Importación de features a la base de datos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso...\n",
      "Proceso finalizado...                                                                                       \n",
      "CPU times: user 16.8 s, sys: 4.41 s, total: 21.2 s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import mysql.connector\n",
    "import csv\n",
    "import os\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "   host=\"localhost\",\n",
    "   database=\"tesis2\",\n",
    "   user=\"tesis2\",\n",
    "   passwd=\"tesis2\"\n",
    ")\n",
    "\n",
    "#adaptado de https://github.com/gbgolding/crema/blob/master/bin/featuresetup_module.py\n",
    "def transcript_info_dict(id_especie, blast_file, cursor, flg_pct):\n",
    "    transcript_dict = {}\n",
    "    with open(blast_file, \"r\") as f:\n",
    "        tab_reader = csv.reader(f, delimiter=(\"\\t\"))\n",
    "        line_1 = next(tab_reader)\n",
    "        first = line_1[0]\n",
    "        score = [float(line_1[9])]\n",
    "        with_len = [[first, float(line_1[1]), float(line_1[2]), float(line_1[3]), float(line_1[9])]] # name identity length frame score\n",
    "        for row in tab_reader:\n",
    "            if row[0] == first:\n",
    "                score.append(float(row[9]))\n",
    "                with_len.append([row[0], float(row[1]), float(row[2]), float(row[3]), float(row[9])])\n",
    "            else:\n",
    "                transcript_dict[first] = {}\n",
    "                transcript_dict[first][\"identity\"] = float(0)\n",
    "                transcript_dict[first][\"align_length\"] = float(0)\n",
    "                max_value = max(score)\n",
    "                max_index = score.index(max_value)\n",
    "                max_len_ident = with_len[max_index]\n",
    "                if max_len_ident[3] > 0:\n",
    "                    transcript_dict[first][\"identity\"] = float(max_len_ident[1])\n",
    "                    transcript_dict[first][\"align_length\"] = float(max_len_ident[2])\n",
    "                score = [float(row[9])]\n",
    "                first = row[0]\n",
    "                with_len = [[first, float(row[1]), float(row[2]), float(row[3]), float(row[9])]]\n",
    "        transcript_dict[first] = {}\n",
    "        transcript_dict[first][\"identity\"] = float(0)\n",
    "        transcript_dict[first][\"align_length\"] = float(0)\n",
    "        max_value = max(score)\n",
    "        max_index = score.index(max_value)\n",
    "        max_len_ident = with_len[max_index]\n",
    "        if max_len_ident[3] > 0:\n",
    "            transcript_dict[first][\"identity\"] = float(max_len_ident[1])\n",
    "            transcript_dict[first][\"align_length\"] = float(max_len_ident[2])\n",
    "    #fin de código adaptado de https://github.com/gbgolding/crema/blob/master/bin/featuresetup_module.py\n",
    "    query = \"SELECT cod_secuencia, IFNULL(longitud, 0), IFNULL(orf_length, 0) FROM secuencias_features WHERE flg_seleccionado = 1 AND id_especie = %s AND flg_pct = %s\"\n",
    "    cursor.execute(query, (id_especie, flg_pct))\n",
    "    transcriptomas = cursor.fetchall()\n",
    "    for transcriptoma in transcriptomas:\n",
    "        cod_secuencia = transcriptoma[0]\n",
    "        longitud = transcriptoma[1]\n",
    "        orf_length = transcriptoma[2]\n",
    "        if transcriptoma[0] in transcript_dict:\n",
    "            transcript_dict[cod_secuencia][\"align_perc_len\"] = float(transcript_dict[cod_secuencia][\"align_length\"]/longitud)\n",
    "            if orf_length == 0:\n",
    "                transcript_dict[cod_secuencia][\"align_perc_ORF\"] = float(0)\n",
    "            else:\n",
    "                transcript_dict[cod_secuencia][\"align_perc_ORF\"] = float(transcript_dict[cod_secuencia][\"align_length\"]/orf_length)\n",
    "        else:\n",
    "            transcript_dict[cod_secuencia] = {}\n",
    "            transcript_dict[cod_secuencia][\"identity\"] = float(0)\n",
    "            transcript_dict[cod_secuencia][\"align_length\"] = float(0)\n",
    "            transcript_dict[cod_secuencia][\"align_perc_len\"] = float(0)\n",
    "            transcript_dict[cod_secuencia][\"align_perc_ORF\"] = float(0)\n",
    "        query = \"UPDATE secuencias_features SET alignment = %s, alignment_length = %s, alignment_proportion = %s, alignment_proportion_orf = %s WHERE id_especie = %s AND cod_secuencia = %s\"\n",
    "        cursor.execute(query, (transcript_dict[cod_secuencia][\"identity\"], transcript_dict[cod_secuencia][\"align_length\"], transcript_dict[cod_secuencia][\"align_perc_len\"], transcript_dict[cod_secuencia][\"align_perc_ORF\"], id_especie, cod_secuencia))\n",
    "\n",
    "print(\"Iniciando proceso...\") \n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "output_directory = '../Semana 05/diamond_features/'\n",
    "\n",
    "query = \"SELECT m.id_especie, REPLACE(m.especie, ' ', '_') FROM maestra_especies m JOIN especies_seleccionadas es ON m.especie = es.especie\"\n",
    "cursor.execute(query)\n",
    "especies = cursor.fetchall()\n",
    "for especie in especies:\n",
    "    print(\"\\rExportando features para \" + str(especie[0]), \" PCT                                      \", end=\"\")\n",
    "    salida = output_directory + especie[1] + \".PCT.tsv\"\n",
    "    transcript_info_dict(especie[0], salida, cursor, 1)\n",
    "    conn.commit()\n",
    "    print(\"\\rExportando features para \" + str(especie[0]), \" lncRNA                                      \", end=\"\")\n",
    "    salida = output_directory + especie[1] + \".lncRNA.tsv\"\n",
    "    transcript_info_dict(especie[0], salida, cursor, 0)\n",
    "    conn.commit()\n",
    "    \n",
    "conn.close()\n",
    "\n",
    "print(\"\\rProceso finalizado...                                                                                       \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.7.Longitud del alineamiento en la base de datos SwissProt</h2>\n",
    "Ya se proceso con el algoritmo diamond.<br>\n",
    "<h2>1.8.Proporción de la longitud del alineamiento con respecto a la longitud del transcriptoma</h2>\n",
    "Ya se proceso con el algoritmo diamond.<br>\n",
    "<h2>1.9.Proporción de la longitud del alineamiento y la longitud del ORF</h2>\n",
    "Ya se proceso con el algoritmo diamond.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
